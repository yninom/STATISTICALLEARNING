{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed Dating Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was gathered from participants in experimental speed dating events from 2002-2004. During the events, the attendees would have a four-minute \"first date\" with every other participant of the opposite sex. At the end of their four minutes, participants were asked if they would like to see their date again. They were also asked to rate their date on six attributes: Attractiveness, Sincerity, Intelligence, Fun, Ambition, and Shared Interests. The dataset also includes questionnaire data gathered from participants at different points in the process. These fields include: demographics, dating habits, self-perception across key attributes, beliefs on what others find valuable in a mate, and lifestyle information.\n",
    "\n",
    "There are 122 columns(independent variables) in the dataset, match column(dependent variable) needs to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_null</th>\n",
       "      <th>wave</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>age_o</th>\n",
       "      <th>d_age</th>\n",
       "      <th>d_d_age</th>\n",
       "      <th>race</th>\n",
       "      <th>race_o</th>\n",
       "      <th>samerace</th>\n",
       "      <th>...</th>\n",
       "      <th>d_expected_num_interested_in_me</th>\n",
       "      <th>d_expected_num_matches</th>\n",
       "      <th>like</th>\n",
       "      <th>guess_prob_liked</th>\n",
       "      <th>d_like</th>\n",
       "      <th>d_guess_prob_liked</th>\n",
       "      <th>met</th>\n",
       "      <th>decision</th>\n",
       "      <th>decision_o</th>\n",
       "      <th>match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b''</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>b'[4-6]'</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>b'European/Caucasian-American'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[0-3]'</td>\n",
       "      <td>b'[3-5]'</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>b'[6-8]'</td>\n",
       "      <td>b'[5-6]'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b''</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'[0-1]'</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>b'European/Caucasian-American'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[0-3]'</td>\n",
       "      <td>b'[3-5]'</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>b'[6-8]'</td>\n",
       "      <td>b'[5-6]'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b''</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'[0-1]'</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[0-3]'</td>\n",
       "      <td>b'[3-5]'</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>b'[6-8]'</td>\n",
       "      <td>b'[0-4]'</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b''</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>b'[2-3]'</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>b'European/Caucasian-American'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[0-3]'</td>\n",
       "      <td>b'[3-5]'</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>b'[6-8]'</td>\n",
       "      <td>b'[5-6]'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b''</td>\n",
       "      <td>1.0</td>\n",
       "      <td>b'female'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>b'[2-3]'</td>\n",
       "      <td>b'Asian/Pacific Islander/Asian-American'</td>\n",
       "      <td>b'Latino/Hispanic American'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>...</td>\n",
       "      <td>b'[0-3]'</td>\n",
       "      <td>b'[3-5]'</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>b'[6-8]'</td>\n",
       "      <td>b'[5-6]'</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  has_null  wave     gender   age  age_o  d_age   d_d_age  \\\n",
       "0      b''   1.0  b'female'  21.0   27.0    6.0  b'[4-6]'   \n",
       "1      b''   1.0  b'female'  21.0   22.0    1.0  b'[0-1]'   \n",
       "2      b''   1.0  b'female'  21.0   22.0    1.0  b'[0-1]'   \n",
       "3      b''   1.0  b'female'  21.0   23.0    2.0  b'[2-3]'   \n",
       "4      b''   1.0  b'female'  21.0   24.0    3.0  b'[2-3]'   \n",
       "\n",
       "                                       race  \\\n",
       "0  b'Asian/Pacific Islander/Asian-American'   \n",
       "1  b'Asian/Pacific Islander/Asian-American'   \n",
       "2  b'Asian/Pacific Islander/Asian-American'   \n",
       "3  b'Asian/Pacific Islander/Asian-American'   \n",
       "4  b'Asian/Pacific Islander/Asian-American'   \n",
       "\n",
       "                                     race_o samerace  ...  \\\n",
       "0            b'European/Caucasian-American'     b'0'  ...   \n",
       "1            b'European/Caucasian-American'     b'0'  ...   \n",
       "2  b'Asian/Pacific Islander/Asian-American'     b'1'  ...   \n",
       "3            b'European/Caucasian-American'     b'0'  ...   \n",
       "4               b'Latino/Hispanic American'     b'0'  ...   \n",
       "\n",
       "   d_expected_num_interested_in_me  d_expected_num_matches like  \\\n",
       "0                         b'[0-3]'                b'[3-5]'  7.0   \n",
       "1                         b'[0-3]'                b'[3-5]'  7.0   \n",
       "2                         b'[0-3]'                b'[3-5]'  7.0   \n",
       "3                         b'[0-3]'                b'[3-5]'  7.0   \n",
       "4                         b'[0-3]'                b'[3-5]'  6.0   \n",
       "\n",
       "  guess_prob_liked    d_like  d_guess_prob_liked  met  decision  decision_o  \\\n",
       "0              6.0  b'[6-8]'            b'[5-6]'  0.0      b'1'        b'0'   \n",
       "1              5.0  b'[6-8]'            b'[5-6]'  1.0      b'1'        b'0'   \n",
       "2              NaN  b'[6-8]'            b'[0-4]'  1.0      b'1'        b'1'   \n",
       "3              6.0  b'[6-8]'            b'[5-6]'  0.0      b'1'        b'1'   \n",
       "4              6.0  b'[6-8]'            b'[5-6]'  0.0      b'1'        b'1'   \n",
       "\n",
       "   match  \n",
       "0   b'0'  \n",
       "1   b'0'  \n",
       "2   b'1'  \n",
       "3   b'1'  \n",
       "4   b'1'  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dating = pd.read_csv('speeddating.csv')\n",
    "dating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8378 entries, 0 to 8377\n",
      "Columns: 123 entries, has_null to match\n",
      "dtypes: float64(59), object(64)\n",
      "memory usage: 7.9+ MB\n"
     ]
    }
   ],
   "source": [
    "dating.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8378, 123)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object     64\n",
       "float64    59\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data types in the features\n",
    "dating.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "has_null                        1\n",
       "decision                        2\n",
       "decision_o                      2\n",
       "samerace                        2\n",
       "match                           2\n",
       "                             ... \n",
       "shared_interests_important     85\n",
       "attractive_important           94\n",
       "pref_o_attractive              94\n",
       "interests_correlate           155\n",
       "field                         260\n",
       "Length: 123, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking for columns with unique values\n",
    "dating.nunique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the column has_null because it has only one value for all the rows\n",
    "dating.drop(['has_null'], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that removes all the unwanted characters b', ''\n",
    "def remove_characters(feature):\n",
    "    return feature.replace(\"b'\",'').replace(\"'\",\"\")\n",
    "\n",
    "#select string columns\n",
    "string_dataset = dating.select_dtypes(include = ['object'])\n",
    "\n",
    "#remove the characters\n",
    "for feature in string_dataset.columns:\n",
    "    dating[feature] = dating[feature].apply(lambda x: remove_characters(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the columns with the prefix d_ are the values of other columns but binned\n",
    "to_drop = [column_name for column_name in dating.columns if column_name.startswith('d_')]\n",
    "dating.drop(to_drop, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8378, 66)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision and decision_o at night event are basically the same as match, match it is calculated from them\n",
    "dating.drop(['decision', 'decision_o'], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_columns = dating.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the columns that have more than the 5% of missing values\n",
    "dating.drop(['expected_num_interested_in_me', 'expected_num_matches', 'shared_interests_o',\n",
    "             'shared_interests_partner', 'ambitous_o', 'ambition_partner'], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There's to columns of age, from self and o, we can get just one age column from diff = self - o\n",
    "dating['age_diff'] = dating['age'] - dating['age_o']\n",
    "dating.drop(['age','age_o'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    7644\n",
       "1.0     351\n",
       "7.0       3\n",
       "5.0       2\n",
       "3.0       1\n",
       "8.0       1\n",
       "6.0       1\n",
       "Name: met, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if they had met the partner before, yes or no (1 or 0)\n",
    "dating['met'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I change the few different values for the mode that is 0\n",
    "for number in [3.0, 5.0, 6.0, 7.0, 8.0]:\n",
    "    dating['met'].replace(number,0, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    7652\n",
       "1.0     351\n",
       "Name: met, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating['met'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#field has many different categorical values, when I convert this column into a numeric one, it sums more than 200 columns\n",
    "dating.drop(['field'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8378, 56)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     7079\n",
       "1      627\n",
       "2      143\n",
       "11     119\n",
       "3       85\n",
       "4       61\n",
       "7       58\n",
       "8       54\n",
       "33      48\n",
       "5       37\n",
       "32      15\n",
       "6        8\n",
       "34       6\n",
       "19       5\n",
       "9        5\n",
       "13       5\n",
       "12       5\n",
       "44       5\n",
       "10       4\n",
       "15       3\n",
       "37       2\n",
       "18       1\n",
       "43       1\n",
       "39       1\n",
       "40       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_rows=dating.isnull().sum(axis = 1)\n",
    "missing_rows.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dating_clean = dating.dropna()  #drop 15% of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7079, 56)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_clean.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric columns\n",
    "columns_numeric = dating_clean.select_dtypes(include = ['int','float']).columns.tolist()\n",
    "\n",
    "#categorical columns\n",
    "columns_category = dating_clean.select_dtypes(include = ['object']).drop('match', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>race_o</th>\n",
       "      <th>samerace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>Asian/Pacific Islander/Asian-American</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>Asian/Pacific Islander/Asian-American</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>Asian/Pacific Islander/Asian-American</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female</td>\n",
       "      <td>Asian/Pacific Islander/Asian-American</td>\n",
       "      <td>Latino/Hispanic American</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>female</td>\n",
       "      <td>Asian/Pacific Islander/Asian-American</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372</th>\n",
       "      <td>male</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>male</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>Latino/Hispanic American</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>male</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>Other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8376</th>\n",
       "      <td>male</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>Asian/Pacific Islander/Asian-American</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377</th>\n",
       "      <td>male</td>\n",
       "      <td>European/Caucasian-American</td>\n",
       "      <td>Asian/Pacific Islander/Asian-American</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7079 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gender                                   race  \\\n",
       "0     female  Asian/Pacific Islander/Asian-American   \n",
       "1     female  Asian/Pacific Islander/Asian-American   \n",
       "3     female  Asian/Pacific Islander/Asian-American   \n",
       "4     female  Asian/Pacific Islander/Asian-American   \n",
       "5     female  Asian/Pacific Islander/Asian-American   \n",
       "...      ...                                    ...   \n",
       "8372    male            European/Caucasian-American   \n",
       "8373    male            European/Caucasian-American   \n",
       "8374    male            European/Caucasian-American   \n",
       "8376    male            European/Caucasian-American   \n",
       "8377    male            European/Caucasian-American   \n",
       "\n",
       "                                     race_o samerace  \n",
       "0               European/Caucasian-American        0  \n",
       "1               European/Caucasian-American        0  \n",
       "3               European/Caucasian-American        0  \n",
       "4                  Latino/Hispanic American        0  \n",
       "5               European/Caucasian-American        0  \n",
       "...                                     ...      ...  \n",
       "8372            European/Caucasian-American        1  \n",
       "8373               Latino/Hispanic American        0  \n",
       "8374                                  Other        0  \n",
       "8376  Asian/Pacific Islander/Asian-American        0  \n",
       "8377  Asian/Pacific Islander/Asian-American        0  \n",
       "\n",
       "[7079 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_clean[columns_category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wave</th>\n",
       "      <th>importance_same_race</th>\n",
       "      <th>importance_same_religion</th>\n",
       "      <th>pref_o_attractive</th>\n",
       "      <th>pref_o_sincere</th>\n",
       "      <th>pref_o_intelligence</th>\n",
       "      <th>pref_o_funny</th>\n",
       "      <th>pref_o_ambitious</th>\n",
       "      <th>pref_o_shared_interests</th>\n",
       "      <th>attractive_o</th>\n",
       "      <th>...</th>\n",
       "      <th>concerts</th>\n",
       "      <th>music</th>\n",
       "      <th>shopping</th>\n",
       "      <th>yoga</th>\n",
       "      <th>interests_correlate</th>\n",
       "      <th>expected_happy_with_sd_people</th>\n",
       "      <th>like</th>\n",
       "      <th>guess_prob_liked</th>\n",
       "      <th>met</th>\n",
       "      <th>age_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372</th>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8376</th>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377</th>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7079 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      wave  importance_same_race  importance_same_religion  pref_o_attractive  \\\n",
       "0      1.0                   2.0                       4.0               35.0   \n",
       "1      1.0                   2.0                       4.0               60.0   \n",
       "3      1.0                   2.0                       4.0               30.0   \n",
       "4      1.0                   2.0                       4.0               30.0   \n",
       "5      1.0                   2.0                       4.0               50.0   \n",
       "...    ...                   ...                       ...                ...   \n",
       "8372  21.0                   1.0                       1.0               10.0   \n",
       "8373  21.0                   1.0                       1.0               10.0   \n",
       "8374  21.0                   1.0                       1.0               50.0   \n",
       "8376  21.0                   1.0                       1.0               10.0   \n",
       "8377  21.0                   1.0                       1.0               20.0   \n",
       "\n",
       "      pref_o_sincere  pref_o_intelligence  pref_o_funny  pref_o_ambitious  \\\n",
       "0               20.0                 20.0          20.0               0.0   \n",
       "1                0.0                  0.0          40.0               0.0   \n",
       "3                5.0                 15.0          40.0               5.0   \n",
       "4               10.0                 20.0          10.0              10.0   \n",
       "5                0.0                 30.0          10.0               0.0   \n",
       "...              ...                  ...           ...               ...   \n",
       "8372            15.0                 30.0          20.0              15.0   \n",
       "8373            10.0                 30.0          20.0              10.0   \n",
       "8374            20.0                 10.0           5.0              10.0   \n",
       "8376            25.0                 25.0          10.0              10.0   \n",
       "8377            20.0                 10.0          15.0               5.0   \n",
       "\n",
       "      pref_o_shared_interests  attractive_o  ...  concerts  music  shopping  \\\n",
       "0                         5.0           6.0  ...      10.0    9.0       8.0   \n",
       "1                         0.0           7.0  ...      10.0    9.0       8.0   \n",
       "3                         5.0           7.0  ...      10.0    9.0       8.0   \n",
       "4                        20.0           8.0  ...      10.0    9.0       8.0   \n",
       "5                        10.0           7.0  ...      10.0    9.0       8.0   \n",
       "...                       ...           ...  ...       ...    ...       ...   \n",
       "8372                     10.0           8.0  ...      10.0   10.0       7.0   \n",
       "8373                     15.0          10.0  ...      10.0   10.0       7.0   \n",
       "8374                      5.0           6.0  ...      10.0   10.0       7.0   \n",
       "8376                     20.0           5.0  ...      10.0   10.0       7.0   \n",
       "8377                     30.0           8.0  ...      10.0   10.0       7.0   \n",
       "\n",
       "      yoga  interests_correlate  expected_happy_with_sd_people  like  \\\n",
       "0      1.0                 0.14                            3.0   7.0   \n",
       "1      1.0                 0.54                            3.0   7.0   \n",
       "3      1.0                 0.61                            3.0   7.0   \n",
       "4      1.0                 0.21                            3.0   6.0   \n",
       "5      1.0                 0.25                            3.0   6.0   \n",
       "...    ...                  ...                            ...   ...   \n",
       "8372   3.0                 0.28                           10.0   4.0   \n",
       "8373   3.0                 0.64                           10.0   2.0   \n",
       "8374   3.0                 0.71                           10.0   4.0   \n",
       "8376   3.0                 0.62                           10.0   5.0   \n",
       "8377   3.0                 0.01                           10.0   4.0   \n",
       "\n",
       "      guess_prob_liked  met  age_diff  \n",
       "0                  6.0  0.0      -6.0  \n",
       "1                  5.0  1.0      -1.0  \n",
       "3                  6.0  0.0      -2.0  \n",
       "4                  6.0  0.0      -3.0  \n",
       "5                  5.0  0.0      -4.0  \n",
       "...                ...  ...       ...  \n",
       "8372               4.0  0.0       1.0  \n",
       "8373               5.0  0.0      -1.0  \n",
       "8374               4.0  0.0       1.0  \n",
       "8376               5.0  0.0       3.0  \n",
       "8377               5.0  0.0       3.0  \n",
       "\n",
       "[7079 rows x 51 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_clean[columns_numeric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use get dummies to convert categorical attributes into numericals\n",
    "dating_ready = pd.get_dummies(data=dating_clean, columns=['gender', 'race', 'race_o', 'samerace', 'match'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7079, 62)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_ready.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_ready.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    51\n",
       "uint8      11\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_ready.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wave</th>\n",
       "      <th>importance_same_race</th>\n",
       "      <th>importance_same_religion</th>\n",
       "      <th>pref_o_attractive</th>\n",
       "      <th>pref_o_sincere</th>\n",
       "      <th>pref_o_intelligence</th>\n",
       "      <th>pref_o_funny</th>\n",
       "      <th>pref_o_ambitious</th>\n",
       "      <th>pref_o_shared_interests</th>\n",
       "      <th>attractive_o</th>\n",
       "      <th>...</th>\n",
       "      <th>race_Black/African American</th>\n",
       "      <th>race_European/Caucasian-American</th>\n",
       "      <th>race_Latino/Hispanic American</th>\n",
       "      <th>race_Other</th>\n",
       "      <th>race_o_Black/African American</th>\n",
       "      <th>race_o_European/Caucasian-American</th>\n",
       "      <th>race_o_Latino/Hispanic American</th>\n",
       "      <th>race_o_Other</th>\n",
       "      <th>samerace_1</th>\n",
       "      <th>match_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.00000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.000000</td>\n",
       "      <td>7079.00000</td>\n",
       "      <td>7079.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.299336</td>\n",
       "      <td>3.782738</td>\n",
       "      <td>3.65772</td>\n",
       "      <td>22.232585</td>\n",
       "      <td>17.444366</td>\n",
       "      <td>20.304576</td>\n",
       "      <td>17.490668</td>\n",
       "      <td>10.723546</td>\n",
       "      <td>11.848880</td>\n",
       "      <td>6.209549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047606</td>\n",
       "      <td>0.563921</td>\n",
       "      <td>0.077977</td>\n",
       "      <td>0.067382</td>\n",
       "      <td>0.048736</td>\n",
       "      <td>0.560107</td>\n",
       "      <td>0.080096</td>\n",
       "      <td>0.065970</td>\n",
       "      <td>0.40048</td>\n",
       "      <td>0.174318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.957994</td>\n",
       "      <td>2.832566</td>\n",
       "      <td>2.81831</td>\n",
       "      <td>12.372573</td>\n",
       "      <td>6.932509</td>\n",
       "      <td>6.831764</td>\n",
       "      <td>6.092708</td>\n",
       "      <td>6.107862</td>\n",
       "      <td>6.348855</td>\n",
       "      <td>1.939503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212945</td>\n",
       "      <td>0.495932</td>\n",
       "      <td>0.268155</td>\n",
       "      <td>0.250701</td>\n",
       "      <td>0.215330</td>\n",
       "      <td>0.496409</td>\n",
       "      <td>0.271461</td>\n",
       "      <td>0.248247</td>\n",
       "      <td>0.49003</td>\n",
       "      <td>0.379410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.520000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>18.370000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.640000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>23.810000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              wave  importance_same_race  importance_same_religion  \\\n",
       "count  7079.000000           7079.000000                7079.00000   \n",
       "mean     11.299336              3.782738                   3.65772   \n",
       "std       5.957994              2.832566                   2.81831   \n",
       "min       1.000000              0.000000                   1.00000   \n",
       "25%       7.000000              1.000000                   1.00000   \n",
       "50%      11.000000              3.000000                   3.00000   \n",
       "75%      15.000000              6.000000                   6.00000   \n",
       "max      21.000000             10.000000                  10.00000   \n",
       "\n",
       "       pref_o_attractive  pref_o_sincere  pref_o_intelligence  pref_o_funny  \\\n",
       "count        7079.000000     7079.000000          7079.000000   7079.000000   \n",
       "mean           22.232585       17.444366            20.304576     17.490668   \n",
       "std            12.372573        6.932509             6.831764      6.092708   \n",
       "min             0.000000        0.000000             0.000000      0.000000   \n",
       "25%            15.000000       15.000000            17.500000     15.000000   \n",
       "50%            20.000000       18.370000            20.000000     18.000000   \n",
       "75%            25.000000       20.000000            23.810000     20.000000   \n",
       "max           100.000000       47.000000            50.000000     50.000000   \n",
       "\n",
       "       pref_o_ambitious  pref_o_shared_interests  attractive_o  ...  \\\n",
       "count       7079.000000              7079.000000   7079.000000  ...   \n",
       "mean          10.723546                11.848880      6.209549  ...   \n",
       "std            6.107862                 6.348855      1.939503  ...   \n",
       "min            0.000000                 0.000000      0.000000  ...   \n",
       "25%            5.000000                 9.520000      5.000000  ...   \n",
       "50%           10.000000                10.640000      6.000000  ...   \n",
       "75%           15.000000                16.000000      8.000000  ...   \n",
       "max           53.000000                30.000000     10.500000  ...   \n",
       "\n",
       "       race_Black/African American  race_European/Caucasian-American  \\\n",
       "count                  7079.000000                       7079.000000   \n",
       "mean                      0.047606                          0.563921   \n",
       "std                       0.212945                          0.495932   \n",
       "min                       0.000000                          0.000000   \n",
       "25%                       0.000000                          0.000000   \n",
       "50%                       0.000000                          1.000000   \n",
       "75%                       0.000000                          1.000000   \n",
       "max                       1.000000                          1.000000   \n",
       "\n",
       "       race_Latino/Hispanic American   race_Other  \\\n",
       "count                    7079.000000  7079.000000   \n",
       "mean                        0.077977     0.067382   \n",
       "std                         0.268155     0.250701   \n",
       "min                         0.000000     0.000000   \n",
       "25%                         0.000000     0.000000   \n",
       "50%                         0.000000     0.000000   \n",
       "75%                         0.000000     0.000000   \n",
       "max                         1.000000     1.000000   \n",
       "\n",
       "       race_o_Black/African American  race_o_European/Caucasian-American  \\\n",
       "count                    7079.000000                         7079.000000   \n",
       "mean                        0.048736                            0.560107   \n",
       "std                         0.215330                            0.496409   \n",
       "min                         0.000000                            0.000000   \n",
       "25%                         0.000000                            0.000000   \n",
       "50%                         0.000000                            1.000000   \n",
       "75%                         0.000000                            1.000000   \n",
       "max                         1.000000                            1.000000   \n",
       "\n",
       "       race_o_Latino/Hispanic American  race_o_Other  samerace_1      match_1  \n",
       "count                      7079.000000   7079.000000  7079.00000  7079.000000  \n",
       "mean                          0.080096      0.065970     0.40048     0.174318  \n",
       "std                           0.271461      0.248247     0.49003     0.379410  \n",
       "min                           0.000000      0.000000     0.00000     0.000000  \n",
       "25%                           0.000000      0.000000     0.00000     0.000000  \n",
       "50%                           0.000000      0.000000     0.00000     0.000000  \n",
       "75%                           0.000000      0.000000     1.00000     0.000000  \n",
       "max                           1.000000      1.000000     1.00000     1.000000  \n",
       "\n",
       "[8 rows x 62 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_ready.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['wave', 'importance_same_race', 'importance_same_religion',\n",
       "       'pref_o_attractive', 'pref_o_sincere', 'pref_o_intelligence',\n",
       "       'pref_o_funny', 'pref_o_ambitious', 'pref_o_shared_interests',\n",
       "       'attractive_o', 'sinsere_o', 'intelligence_o', 'funny_o',\n",
       "       'attractive_important', 'sincere_important', 'intellicence_important',\n",
       "       'funny_important', 'ambtition_important', 'shared_interests_important',\n",
       "       'attractive', 'sincere', 'intelligence', 'funny', 'ambition',\n",
       "       'attractive_partner', 'sincere_partner', 'intelligence_partner',\n",
       "       'funny_partner', 'sports', 'tvsports', 'exercise', 'dining', 'museums',\n",
       "       'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv', 'theater',\n",
       "       'movies', 'concerts', 'music', 'shopping', 'yoga',\n",
       "       'interests_correlate', 'expected_happy_with_sd_people', 'like',\n",
       "       'guess_prob_liked', 'met', 'age_diff', 'gender_male',\n",
       "       'race_Black/African American', 'race_European/Caucasian-American',\n",
       "       'race_Latino/Hispanic American', 'race_Other',\n",
       "       'race_o_Black/African American', 'race_o_European/Caucasian-American',\n",
       "       'race_o_Latino/Hispanic American', 'race_o_Other', 'samerace_1',\n",
       "       'match_1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_ready.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose attributes to make the regression and the target\n",
    "y = dating_ready[['match_1']]\n",
    "X = dating_ready.drop(['match_1'], axis = 1) #all the attributes\n",
    "X1 = X[['like', 'met']] \n",
    "X2 = X[['shopping', 'concerts', 'clubbing']]\n",
    "X3 = X[['sports', 'tvsports', 'hiking', 'exercise']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, test_size=0.3, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.3, random_state=42)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X: 0.8483992467043314\n",
      "Cross validation X:  [0.82744702 0.83047427 0.85570131 0.85671039 0.85368315]\n",
      "Mean X:  0.844803229061554\n"
     ]
    }
   ],
   "source": [
    "#all the attributes, cv = 5 means we split the data en 5 folds\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "\n",
    "training_X = log_reg.fit(X_train, y_train)\n",
    "test_X = log_reg.score(X_test, y_test)\n",
    "\n",
    "result_X = cross_val_score(log_reg, X_train, y_train, cv = 5)\n",
    "\n",
    "print(\"Test X:\" , test_X)\n",
    "print(\"Cross validation X: \",result_X)\n",
    "print(\"Mean X: \",result_X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X1: 0.8225047080979284\n",
      "Cross validation X1:  [0.82240161 0.82946519 0.82744702 0.83047427 0.81634712]\n",
      "Mean X1:  0.8252270433905146\n"
     ]
    }
   ],
   "source": [
    "#all the attributes, cv = 5 means we split the data en 5 folds\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "\n",
    "training_X1 = log_reg.fit(X1_train, y1_train)\n",
    "test_X1 = log_reg.score(X1_test, y1_test)\n",
    "\n",
    "result_X1 = cross_val_score(log_reg, X1_train, y1_train, cv = 5)\n",
    "\n",
    "print(\"Test X1:\" , test_X1)\n",
    "print(\"Cross validation X1: \",result_X1)\n",
    "print(\"Mean X1: \",result_X1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X2: 0.8220338983050848\n",
      "Cross validation X2:  [0.82744702 0.82744702 0.82744702 0.82744702 0.82643794]\n",
      "Mean X2:  0.827245206861756\n"
     ]
    }
   ],
   "source": [
    "#all the attributes, cv = 5 means we split the data en 5 folds\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "\n",
    "training_X2 = log_reg.fit(X2_train, y2_train)\n",
    "test_X2 = log_reg.score(X2_test, y2_test)\n",
    "\n",
    "result_X2 = cross_val_score(log_reg, X2_train, y2_train, cv = 5)\n",
    "\n",
    "print(\"Test X2:\" , test_X2)\n",
    "print(\"Cross validation X2: \",result_X2)\n",
    "print(\"Mean X2: \",result_X2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X3: 0.8220338983050848\n",
      "Cross validation X3:  [0.82744702 0.82744702 0.82744702 0.82744702 0.82643794]\n",
      "Mean X3:  0.827245206861756\n"
     ]
    }
   ],
   "source": [
    "#all the attributes, cv = 5 means we split the data en 5 folds\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "\n",
    "training_X3 = log_reg.fit(X3_train, y3_train)\n",
    "test_X3 = log_reg.score(X3_test, y3_test)\n",
    "\n",
    "result_X3 = cross_val_score(log_reg, X3_train, y3_train, cv = 5)\n",
    "\n",
    "print(\"Test X3:\" , test_X3)\n",
    "print(\"Cross validation X3: \",result_X3)\n",
    "print(\"Mean X3: \",result_X3.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X: 0.7777777777777778\n",
      "Cross validation X:  [0.76387487 0.76185671 0.790111   0.78203835 0.79414733]\n",
      "Mean X:  0.7784056508577194\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#all the attributes, cv = 5 means we split the data en 5 folds\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "training_X = tree.fit(X_train, y_train)\n",
    "test_X = tree.score(X_test, y_test)\n",
    "\n",
    "result_X = cross_val_score(tree, X_train, y_train, cv = 5)\n",
    "\n",
    "print(\"Test X:\" , test_X)\n",
    "print(\"Cross validation X: \",result_X)\n",
    "print(\"Mean X: \",result_X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X1: 0.8253295668549906\n",
      "Cross validation X1:  [0.82542886 0.82946519 0.83047427 0.82542886 0.82542886]\n",
      "Mean X1:  0.827245206861756\n"
     ]
    }
   ],
   "source": [
    "#all the attributes, cv = 5 means we split the data en 5 folds\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "training_X1 = tree.fit(X1_train, y1_train)\n",
    "test_X1 = tree.score(X1_test, y1_test)\n",
    "\n",
    "result_X1 = cross_val_score(tree, X1_train, y1_train, cv = 5)\n",
    "\n",
    "print(\"Test X1:\" , test_X1)\n",
    "print(\"Cross validation X1: \",result_X1)\n",
    "print(\"Mean X1: \",result_X1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X2: 0.821563088512241\n",
      "Cross validation X2:  [0.82542886 0.81634712 0.8284561  0.82139253 0.82240161]\n",
      "Mean X2:  0.8228052472250253\n"
     ]
    }
   ],
   "source": [
    "#all the attributes, cv = 5 means we split the data en 5 folds\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "training_X2 = tree.fit(X2_train, y2_train)\n",
    "test_X2 = tree.score(X2_test, y2_test)\n",
    "\n",
    "result_X2 = cross_val_score(tree, X2_train, y2_train, cv = 5)\n",
    "\n",
    "print(\"Test X2:\" , test_X2)\n",
    "print(\"Cross validation X2: \",result_X2)\n",
    "print(\"Mean X2: \",result_X2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X3: 0.821563088512241\n",
      "Cross validation X3:  [0.82643794 0.80726539 0.82542886 0.81432896 0.82139253]\n",
      "Mean X3:  0.8189707366296671\n"
     ]
    }
   ],
   "source": [
    "#all the attributes, cv = 5 means we split the data en 5 folds\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "training_X3 = tree.fit(X3_train, y3_train)\n",
    "test_X3 = tree.score(X3_test, y3_test)\n",
    "\n",
    "result_X3 = cross_val_score(tree, X3_train, y3_train, cv = 5)\n",
    "\n",
    "print(\"Test X3:\" , test_X3)\n",
    "print(\"Cross validation X3: \",result_X3)\n",
    "print(\"Mean X3: \",result_X3.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for the hyperparameters of the decision tree: criterion, min_samples_leaf, max_depth and random_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for X:  {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 50, 'random_state': 0}\n",
      "Mean for X:  0.8437941473259334\n",
      "Best parameters for X1:  {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 5, 'random_state': 0}\n",
      "Mean for X1:  0.8286579212916246\n",
      "Best parameters for X2:  {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 5, 'random_state': 0}\n",
      "Mean for X2:  0.8286579212916246\n",
      "Best parameters for X3:  {'criterion': 'gini', 'max_depth': 2, 'min_samples_leaf': 5, 'random_state': 0}\n",
      "Mean for X3:  0.827245206861756\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'criterion': ['gini', 'entropy'], 'min_samples_leaf': [5, 10, 50, 100, 150, 200],\n",
    "              'max_depth': [2, 4, 6, 8, 10, 12], 'random_state': [0, 10, 42]}\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "searching_X = GridSearchCV(tree, parameters, cv=5)\n",
    "searching_X.fit(X_train, y_train)\n",
    "\n",
    "searching_X1 = GridSearchCV(tree, parameters, cv=5)\n",
    "searching_X1.fit(X1_train, y1_train)\n",
    "\n",
    "searching_X2 = GridSearchCV(tree, parameters, cv=5)\n",
    "searching_X2.fit(X2_train, y2_train)\n",
    "\n",
    "searching_X3 = GridSearchCV(tree, parameters, cv=5)\n",
    "searching_X3.fit(X3_train, y3_train)\n",
    "\n",
    "\n",
    "print(\"Best parameters for X: \", searching_X.best_params_)\n",
    "print(\"Mean for X: \", searching_X.best_score_)\n",
    "\n",
    "print(\"Best parameters for X1: \", searching_X1.best_params_)\n",
    "print(\"Mean for X1: \", searching_X1.best_score_)\n",
    "\n",
    "print(\"Best parameters for X2: \", searching_X2.best_params_)\n",
    "print(\"Mean for X2: \", searching_X2.best_score_)\n",
    "\n",
    "print(\"Best parameters for X3: \", searching_X3.best_params_)\n",
    "print(\"Mean for X3: \", searching_X3.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X: 0.853578154425612\n",
      "Cross validation X:  [0.84460141 0.85267407 0.8506559  0.84964682 0.85469223]\n",
      "Mean X:  0.8504540867810292\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#all the attributes, cv = 5 means we split the data en 5 folds\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "\n",
    "training_X = forest.fit(X_train, y_train)\n",
    "test_X = forest.score(X_test, y_test)\n",
    "\n",
    "result_X = cross_val_score(forest, X_train, y_train, cv = 5)\n",
    "\n",
    "print(\"Test X:\" , test_X)\n",
    "print(\"Cross validation X: \",result_X)\n",
    "print(\"Mean X: \",result_X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X1: 0.8253295668549906\n",
      "Cross validation X1:  [0.8284561  0.82946519 0.83047427 0.82744702 0.82542886]\n",
      "Mean X1:  0.8282542885973763\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(random_state=0)\n",
    "\n",
    "training_X1 = forest.fit(X1_train, y1_train)\n",
    "test_X1 = forest.score(X1_test, y1_test)\n",
    "\n",
    "result_X1 = cross_val_score(forest, X1_train, y1_train, cv = 5)\n",
    "\n",
    "print(\"Test X1:\" , test_X1)\n",
    "print(\"Cross validation X1: \",result_X1)\n",
    "print(\"Mean X1: \",result_X1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X2: 0.82015065913371\n",
      "Cross validation X2:  [0.82643794 0.80928355 0.82744702 0.82038345 0.81735621]\n",
      "Mean X2:  0.8201816347124117\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(random_state=0)\n",
    "\n",
    "training_X2 = forest.fit(X2_train, y2_train)\n",
    "test_X2 = forest.score(X2_test, y2_test)\n",
    "\n",
    "result_X2 = cross_val_score(forest, X2_train, y2_train, cv = 5)\n",
    "\n",
    "print(\"Test X2:\" , test_X2)\n",
    "print(\"Cross validation X2: \",result_X2)\n",
    "print(\"Mean X2: \",result_X2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X3: 0.8182674199623352\n",
      "Cross validation X3:  [0.82542886 0.79818365 0.8234107  0.81029263 0.81533804]\n",
      "Mean X3:  0.8145307769929364\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(random_state=0)\n",
    "\n",
    "training_X3 = forest.fit(X3_train, y3_train)\n",
    "test_X3 = forest.score(X3_test, y3_test)\n",
    "\n",
    "result_X3 = cross_val_score(forest, X3_train, y3_train, cv = 5)\n",
    "\n",
    "print(\"Test X3:\" , test_X3)\n",
    "print(\"Cross validation X3: \",result_X3)\n",
    "print(\"Mean X3: \",result_X3.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for some of the hyperparameters of the random forest: n_estimators, min_samples_leaf, max_features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for X:  {'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "Mean for X:  0.848234106962664\n",
      "Best parameters for X1:  {'max_features': 'sqrt', 'min_samples_leaf': 10, 'n_estimators': 50}\n",
      "Mean for X1:  0.8286579212916246\n",
      "Best parameters for X2:  {'max_features': 'sqrt', 'min_samples_leaf': 10, 'n_estimators': 50}\n",
      "Mean for X2:  0.827245206861756\n",
      "Best parameters for X3:  {'max_features': 'sqrt', 'min_samples_leaf': 50, 'n_estimators': 50}\n",
      "Mean for X3:  0.827245206861756\n"
     ]
    }
   ],
   "source": [
    "parameters = {'min_samples_leaf': [5, 10, 50, 100, 150, 200], \n",
    "              'n_estimators': [50, 100, 150, 200], 'max_features': ['sqrt', 'log2']}\n",
    "\n",
    "forest = RandomForestClassifier(random_state=0)\n",
    "\n",
    "searching_X = GridSearchCV(forest, parameters, cv=5)\n",
    "searching_X.fit(X_train, y_train)\n",
    "searching_X1= GridSearchCV(forest, parameters, cv=5)\n",
    "searching_X1.fit(X1_train, y1_train)\n",
    "searching_X2 = GridSearchCV(forest, parameters, cv=5)\n",
    "searching_X2.fit(X2_train, y_train)\n",
    "searching_X3 = GridSearchCV(forest, parameters, cv=5)\n",
    "searching_X3.fit(X3_train, y_train)\n",
    "\n",
    "print(\"Best parameters for X: \", searching_X.best_params_)\n",
    "print(\"Mean for X: \", searching_X.best_score_)\n",
    "\n",
    "print(\"Best parameters for X1: \", searching_X1.best_params_)\n",
    "print(\"Mean for X1: \", searching_X1.best_score_)\n",
    "\n",
    "print(\"Best parameters for X2: \", searching_X2.best_params_)\n",
    "print(\"Mean for X2: \", searching_X2.best_score_)\n",
    "\n",
    "print(\"Best parameters for X3: \", searching_X3.best_params_)\n",
    "print(\"Mean for X3: \", searching_X3.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X: 0.8408662900188324\n",
      "Cross validation X:  [0.83854692 0.83854692 0.839556   0.84157417 0.84258325]\n",
      "Mean X:  0.8401614530776994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "polynomial = SVC(kernel='poly', random_state = 0)\n",
    "\n",
    "training_X = polynomial.fit(X_train,y_train)\n",
    "test_X = polynomial.score(X_test, y_test)\n",
    "\n",
    "result_X = cross_val_score(polynomial, X_train, y_train, cv = 5)\n",
    "\n",
    "print(\"Test X:\" , test_X)\n",
    "print(\"Cross validation X: \",result_X)\n",
    "print(\"Mean X: \",result_X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X1: 0.8220338983050848\n",
      "Cross validation X1:  [0.82744702 0.82744702 0.82744702 0.82744702 0.82643794]\n",
      "Mean X1:  0.827245206861756\n"
     ]
    }
   ],
   "source": [
    "polynomial = SVC(kernel='poly', random_state = 0)\n",
    "\n",
    "training_X1 = polynomial.fit(X1_train,y1_train)\n",
    "test_X1 = polynomial.score(X1_test, y1_test)\n",
    "\n",
    "result_X1 = cross_val_score(polynomial, X1_train, y1_train, cv = 5)\n",
    "\n",
    "print(\"Test X1:\" , test_X1)\n",
    "print(\"Cross validation X1: \",result_X1)\n",
    "print(\"Mean X1: \",result_X1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X2: 0.8220338983050848\n",
      "Cross validation X2:  [0.82744702 0.82744702 0.82744702 0.82744702 0.82643794]\n",
      "Mean X2:  0.827245206861756\n"
     ]
    }
   ],
   "source": [
    "polynomial = SVC(kernel='poly', random_state = 0)\n",
    "\n",
    "training_X2 = polynomial.fit(X2_train,y2_train)\n",
    "test_X2 = polynomial.score(X2_test, y2_test)\n",
    "\n",
    "result_X2 = cross_val_score(polynomial, X2_train, y2_train, cv = 5)\n",
    "\n",
    "print(\"Test X2:\" , test_X2)\n",
    "print(\"Cross validation X2: \",result_X2)\n",
    "print(\"Mean X2: \",result_X2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test X3: 0.8220338983050848\n",
      "Cross validation X3:  [0.82744702 0.82744702 0.82744702 0.82744702 0.82643794]\n",
      "Mean X3:  0.827245206861756\n"
     ]
    }
   ],
   "source": [
    "polynomial = SVC(kernel='poly', random_state = 0)\n",
    "\n",
    "training_X3 = polynomial.fit(X3_train,y3_train)\n",
    "test_X3 = polynomial.score(X3_test, y3_test)\n",
    "\n",
    "result_X3 = cross_val_score(polynomial, X3_train, y3_train, cv = 5)\n",
    "\n",
    "print(\"Test X3:\" , test_X3)\n",
    "print(\"Cross validation X3: \",result_X3)\n",
    "print(\"Mean X3: \",result_X3.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for the hyperparameters of the polynomial kernel: $\\gamma$, $r$ and $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'degree': [2, 3, 4],\n",
    "              'gamma': [0.1, 1, 10],\n",
    "              'coef0': [0, 1, 2]}\n",
    "\n",
    "polynomial = SVC(kernel='poly', random_state = 0)\n",
    "\n",
    "searching_X = GridSearchCV(polynomial, parameters, cv=5)\n",
    "searching_X.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters for X: \", searching_X.best_params_)\n",
    "print(\"Mean for X: \", searching_X.best_score_)\n",
    "\n",
    "y_predicted = searching_X.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_predicted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features      Log Reg    Log Reg CV    Dec Tree    Dec Tree CV    Dec Tree Hyp    Rand Forest    Rand Forest CV    Rand Forest Hyp    Poly    Poly CV\n",
      "----------  ---------  ------------  ----------  -------------  --------------  -------------  ----------------  -----------------  ------  ---------\n",
      "X               0.848         0.844       0.777          0.778           0.844          0.854             0.85               0.848   0.841      0.84\n",
      "X1              0.822         0.825       0.825          0.827           0.829          0.825             0.828              0.829   0.822      0.827\n",
      "X2              0.822         0.827       0.822          0.823           0.829          0.82              0.82               0.827   0.822      0.827\n",
      "X3              0.822         0.827       0.822          0.819           0.827          0.818             0.814              0.827   0.822      0.827\n"
     ]
    }
   ],
   "source": [
    "# X = dating_ready.drop(['match_1'], axis = 1) #all the attributes\n",
    "# X1 = X[['like', 'met']] \n",
    "# X2 = X[['shopping', 'concerts', 'clubbing']]\n",
    "# X3 = X[['sports', 'tvsports', 'hiking', 'exercise']]\n",
    "\n",
    "from tabulate import tabulate \n",
    "#create data\n",
    "data = [[\"X\", 0.848, 0.844, 0.777, 0.778, 0.844, 0.854, 0.850, 0.848, 0.841, 0.840],\n",
    "        [\"X1\", 0.822, 0.825, 0.825, 0.827, 0.829, 0.825, 0.828, 0.829, 0.822, 0.827],\n",
    "        [\"X2\", 0.822, 0.827, 0.822, 0.823, 0.829, 0.820, 0.820, 0.827, 0.822, 0.827],\n",
    "        [\"X3\", 0.822, 0.827, 0.822, 0.819, 0.827, 0.818, 0.814, 0.827, 0.822, 0.827]\n",
    "        ]\n",
    "  \n",
    "#define header names\n",
    "col_names = [\"Features\", \"Log Reg\", \"Log Reg CV\", \"Dec Tree\", \"Dec Tree CV\", \"Dec Tree Hyp\",\n",
    "             \"Rand Forest\", \"Rand Forest CV\", \"Rand Forest Hyp\", \"Poly\", \"Poly CV\"]\n",
    "  \n",
    "#display table\n",
    "print(tabulate(data, headers=col_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "homeworks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
